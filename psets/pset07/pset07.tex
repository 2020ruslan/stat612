\documentclass[12pt]{article}

\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}}

\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%

\geometry{top=1in, bottom=1in, left=1in, right=1in, marginparsep=4pt, marginparwidth=1in}

\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\cfoot{\thepage\ of \pageref{LastPage}}

\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

\usepackage{marginnote} % For margin years
\newcommand{\years}[1]{\marginnote{\scriptsize #1}} % New command for including margin years
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{-16pt} % Slightly increase the distance of the margin years from the content
\reversemarginpar

\setromanfont [Ligatures={Common}, Numbers={OldStyle}, Variant=01,
 BoldFont={LinLibertine_RB.otf},
 ItalicFont={LinLibertine_RI.otf},
 BoldItalicFont={LinLibertine_RBI.otf}
 ]{LinLibertine_R.otf}
%\setromanfont [Ligatures={Common}, Numbers={OldStyle}]{Hoefler Text}

%\usepackage[xetex, bookmarks, pdftitle={Taylor Arnold CV},pdfauthor={Taylor Arnold}]{hyperref}
%\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=MidnightBlue}

\usepackage{xunicode} % Allows generation of unicode characters from accented glyphs
\defaultfontfeatures{Mapping=tex-text}

\begin{document}

\begin{center}
{\bf Problem Set 07} \\
Linear Models -- Fall 2015 \\
Due date: 2015-12-16
\end{center}

\medskip

Please hand write or type up and print the solutions; we will not accept e-mail solution sets except
in exceptional circumstances. You may discuss problem sets with others, but must
write up your own solutions. This means that you should have no need to look at other's
final written solutions. Many of these problems come from a variety of textbooks,
which are referenced in the problems. These are for citation purposes and not because
you will need to consult the text itself (though you may feel free to do so).

\textbf{Please hand your solutions in to the office at 24 Hillhouse Avenue. There should be
a specific folder for the class. Make sure to deliver it by 4pm on the due date.}

\medskip

\textbf{1.} Consider the following form of the elastic net:
\begin{align*}
R(\beta) &= \frac{1}{2n} \sum_{i=1}^n ( y_i - x^t_i \beta)^2 + \lambda P_\alpha (\beta)
\end{align*}
Where:
\begin{align*}
P_\alpha &= (1 - \alpha) \frac{1}{2} || \beta ||_2^2 + \alpha || \beta ||_1
\end{align*}
Notice that there is a slightly different scaling of the problem from that used in my lecture
notes.

Calculate the partial subdifferential of $R(\beta)$ with respect to $\beta_j$. As in the
lasso case, you will probably need to write this as a conditional on the sign of $\beta_j$.

\textbf{2.} Coordinate descent is a method for finding the minimizer of a multidimensional
convex function. It cyclically optimizes the problem separately over each coordinate until
convergence. So, for example, to solve $R(\beta)$ above, we start with an initial guess of
the solution, $\beta^{(0)}$, and then solve:
\begin{align*}
\beta^{(1)}_1 &= \argmin_b \left\{R(b, \beta^{(0)}_2, \beta^{(0)}_3, \ldots, \beta^{(0)}_p) \right\} \\
\beta^{(1)}_2 &= \argmin_b \left\{R(\beta^{(1)}_1, b, \beta^{(0)}_3, \ldots, \beta^{(0)}_p) \right\} \\
&\vdots \\
\beta^{(1)}_p &= \argmin_b \left\{R(\beta^{(1)}_1, \beta^{(1)}_2, \beta^{(1)}_3, \ldots, b) \right\} \\
\beta^{(2)}_1 &= \argmin_b \left\{R(b, \beta^{(1)}_2, \beta^{(1)}_3, \ldots, \beta^{(1)}_p) \right\} \\
\beta^{(2)}_2 &= \argmin_b \left\{R(\beta^{(2)}_1, b, \beta^{(1)}_3, \ldots, \beta^{(1)}_p) \right\} \\
&\vdots
\end{align*}
Which continues until some $\beta^{(k)}$ is reached that either has converged or reached a maximum
number of iterations.

Write down the formula (you may need multiple to handle different cases depending on signs and
magnitudes) for calculating $\beta^{(k)}_j$. It may be useful to use the notation:
\begin{align*}
\widetilde{\beta}_i &= \left\{ \begin{array}{cc} \beta^{(k+1)}_i \, &i < j \\ \beta^{(k)}_i \, &i \geq j \end{array} \right.
\end{align*}

\textbf{3.} Write an R function $\texttt{getBetaEnet(X, y, alpha, lambda, iter)}$ which
applies coordinate descent to the formula you developed in question 2 to solve the elastic
net for the given $\alpha$ and $\lambda$ values. It should apply coordinate descent to all
the coordinates $\texttt{iter}$ times; you do not need to write a more complex stopping criterion.

\textbf{4.} Take the data from from here:
\begin{quote}
\url{http://www.stat.yale.edu/~tba3/stat612/psets/pset07/data/pset07_X.Rds}
\url{http://www.stat.yale.edu/~tba3/stat612/psets/pset07/data/pset07_y.Rds}
\end{quote}
And run the following:
\begin{verbatim}
> library(glmnet)
> out <- glmnet(X, y, intercept=FALSE, standardize=FALSE, nlambda=5, alpha=0.5)
\end{verbatim}
Apply your function from question $3$ to the 5 values of lambda as given above (use 25 iterations).
Does it produce similar results to \texttt{glmnet}? Compare both the mean squared error
in $\beta$ as well as whether both techniques set the same variables exactly to zero. If they
do not agree, explain why you think that might be the case. If they do agree well, determine the
approximate minimum number of iterations needed to converge for the middle lambda value.

% set.seed(1)
% library(glmnet)
% n <- 1000
% p <- 20
% s <- 5
% X <- matrix(rnorm(n*p),ncol=p)
% X <- X[,1]*0.8 + X*0.2
% beta <- rep(0,p)
% beta[sample(1:p,s)] <- rnorm(s)
% y <- X%*%beta + rnorm(n)
% out <- glmnet(X, y, intercept=FALSE, standardize=FALSE, nlambda=5)
% saveRDS(X, "pset07_X.Rds")
% saveRDS(y, "pset07_y.Rds")




\end{document}





