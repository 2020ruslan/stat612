\input{../header.tex}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 15 \\\vspace{0.2cm}
Ridge Regression and PCR}\\\vspace{0.5cm}
04 November 2015

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics \\
STAT 312/612
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Notes}

\begin{itemize}
\item problem set 5 posted; due next Wednesday \pause
\item problem set 6 will be due in two weeks, November 18th,
  and is also posted (courtesy of DP) \pause
\item the second midterm will be a take-home exam; available online on
  December 2nd, and due December 7th \pause
\item the final problem set, 7, is formally due the last day of
  classes (but we'll accept them through December 14th)
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Goals for today}

\begin{itemize}
\item a note on numerical and statistical noise
\item ridge regression formulation and link to SVD
\item principal component analysis
\item applications to image data
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Statistical noise as numerical noise}

In problem set $5$ you are going to construct the pseudo-inverse $A^{+}$
of an arbitrary matrix $A$ in terms of $A$'s singular value decomposition.

\pause Consider the standard description of a statistical linear model:
\begin{align*}
y &= X \beta + \epsilon
\end{align*}
If we have some sort of inverse of $X$, we can try to write this as:
\begin{align*}
y' &= X (\beta + X^{+} \epsilon)
\end{align*}
And now the error is in $\beta$ rather than in $y$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Statistical noise as numerical noise, cont.}

It turns out that the $y$ values in the second equation will not be exactly
the same as those generated by the original model for the same error terms.
However, the least squares estimate of $\beta$ will be the same.

\pause So, when considering statistical linear models we know that there is
an equivalent problem involving the same $X$ matrix and $\beta$ vector for
which the noise is only due to numerical or measurement error in $\beta$.

\pause This is purely to justify why we care about condition numbers and
linking concepts in numerical analysis with those in statistics. We would
never actually convert the problem to this alternative format, partially
because we cannot without knowledge of the error terms.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Ridge regression}

The ridge regression estimator is the solution to the following
modified least squares optimization problem for some value of $\lambda > 0$.
\begin{align*}
\widehat{\beta}_{ridge} &= \argmin_b \left\{ || y - Xb ||_2^2 + \lambda || b ||_2^2 \right\}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Why the ridge penalty?
\begin{enumerate}
\item The equation shrinks the coefficients towards zero, adding some bias but
reducing the variance of the estimator. \pause
\item Using the $\ell_2$-norm keeps the equation rotationally invariant. \pause
\item Ridge regression has an analytical solution.
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

To see this write the criterion as a matrix equation:
\begin{align*}
(y - Xb)^t (y - Xb)  + \lambda b^t b
&= y^t y + b^t X^t X b - 2 y^t X b + \lambda b^t b \\
\end{align*}
\pause And take its derivative:
\begin{align*}
\frac{\partial}{\partial \beta} \left( y^t y + b^t X^t X b - 2 y^t X b + \lambda b^t b \right)
&= 2 X^t X b - 2 X^t y + 2 \lambda b
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Setting this to zero yields
\begin{align*}
2 X^t X \widehat{\beta} + 2 \lambda \widehat{\beta} &= 2 X^t y \\
(X^t X + I_p \lambda) \widehat{\beta} &=  X^t y \\
\widehat{\beta} &= (X^t X + I_p \lambda)^{-1} \cdot X^t y \\
\end{align*}
\pause This is a useful analytical form, though as with least
squares we would generally not invert the matrix directly but
instead use a stable matrix decomposition.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Now consider the singular value decomposition $U\Sigma V^t$ of
the matrix $X$. \pause We can write the projection matrix $P$ in terms
of this as:
\begin{align*}
P &= X (X^t X)^{-1} X^t \\
&= U \Sigma V^t (V \Sigma^2 V^t)^{-1} V \Sigma U^t \\
&= U \Sigma V^t V \Sigma^{-2} V^t V \Sigma U^t \\
&= U \left[\begin{array}{cc} I_{p} & 0 \\ 0 & 0 \end{array}\right] U^t \\
\end{align*}
\pause This can be written as $UU^t$ if we remember to use the
\textit{thin SVD}.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

The analogue of the projection matrix for ridge regression
is given by:
\begin{align*}
P_{\lambda} &= X (X^t X + \lambda I_p)^{-1} X^t \\
\end{align*}
Where $P_{0}$ is equal to the ordinary $P$. \pause As was
the case last time, this matrix maps $y$ into the predicted
values $\hat{y}$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Notice that because $VV^t$ is equal to the identity matrix, we
can write the inner term of this projection matrix in a nice form:
\begin{align*}
X^t X + \lambda I_p &= V \Sigma^2 V^t + \lambda V V^t \\
&= V(\Sigma^2 + \lambda)V^t
\end{align*}
\pause And the inverse is given as:
\begin{align*}
(X^t X + \lambda I_p)^{-1}
&= V(\Sigma^2 + \lambda)^{-1}V^t \\
&= V E_{\lambda} V^t
\end{align*}
\pause Where $E_{\lambda}$ is a diagonal matrix with entries:
\begin{align*}
E_{\lambda} &=
\text{diag} \left( \frac{1}{\sigma^2_{max} + \lambda}, \ldots, \frac{1}{\sigma^2_{min} + \lambda}\right)
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Remember that the condition number is the ratio of the largest and smallest
singular value of a matrix.\pause What is the condition number of $X^tX$?
\begin{align}
\text{cond} (X^t X) &= \frac{\sigma_{max}^2}{\sigma_{min}^2}
\end{align}
\pause How about the condition number of $X^t X + \lambda I_p$?
\begin{align}
\text{cond} (X^t X + \lambda I_p) &= \frac{\sigma_{max}^2 + \lambda}{\sigma_{min}^2 + \lambda}
\end{align}
\pause How does the incorporation of $\lambda$ change our ability to invert the
matrix?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Back to the projection matrix, what is the decomposition of
$P_\lambda$ in terms of the singular value decomposition? \pause
\begin{align*}
P_\lambda &= X (X^t X + \lambda I_p)^{-1} X^t \\
&= U \Sigma V^t (V^t \Sigma^2 V + \lambda I_p)^{-1} V \Sigma U^t \\
&= U \Sigma V^t {\color{solarized@red} V (\Sigma^2 + \lambda I_p)^{-1} V^t} V \Sigma U^t \\
&= U {\color{solarized@blue} \Sigma (\Sigma^{2} + \lambda I_p)^{-1} \Sigma} U^t \\
&= U D U^t \\
\end{align*}
\pause For the diagonal matrix $D$:
\begin{align}
D &= \text{diag} \left( \frac{\sigma_1^2}{\sigma_1^2 + \lambda}, \ldots, \frac{\sigma_p^2}{\sigma_p^2 + \lambda}  \right)
\end{align}
\pause So we are shrinking in the directions of the singular vectors,
with more shrinkage on the smaller singular values.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Finally, and similarly, we can write the solution $\widehat{\beta}_\lambda$
as:
\begin{align*}
\widehat{\beta}_\lambda
&= V \cdot \text{diag} \left(\frac{\sigma_1}{\sigma_1^2 + \lambda}, \ldots, \frac{\sigma_p}{\sigma_p^2 + \lambda}\right) \cdot U^t y
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Application of ridge to a single photo}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Principal component analysis}

The principal components of the matrix $X$ is a linear
reparameterization $T=XW$ of the matrix $X$ such that: \pause
\begin{enumerate}
\item Each new coordinate is uncorrelated with the others; specifically,
W is an orthogonal matrix called the \textit{loadings} \pause
\item The first component has the largest variance of all
linear combinations of the columns of X, the second has the
highest variance conditioned on being uncorrelated with the
first, and so forth.
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Considering the first column of the matrix $W$, we can write the
condition as follows:
\begin{align*}
\argmax_{w:\, ||w||_2 = 1} \left\{ ||Xw||_2 \right\}
\end{align*}
\pause However, we already know that this is maximized when
$w$ is a multiple of the first right singular vector. That is,
the first column of $V$ in the singular value decomposition $U\Sigma V^t$
of $X$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Likewise, we can argue that the second column of $W$ is the second
column of $V$, and so forth for all of the principal components.

\pause Therefore, the principal components are given by $T = XV$.
This gives:
\begin{align*}
T &= XV \\
&= U\Sigma V^t V \\
&= U \Sigma
\end{align*}
So the components are the weighted columns of the left singular values.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Principal component regression (PCR) uses the first $k$ columns of $T$
as the design matrix, which we will denote $T_k = U_k \Sigma_k$.
\pause The regression vector is then defined as:
\begin{align*}
\widehat{\beta}_{k} &= V_k (T_k^t T_k)^{-1} T_k^t y
\end{align*}
\pause Notice that this can be simplified as:
\begin{align*}
\widehat{\beta}_{k} &= V_k (\Sigma_k U^t U \Sigma_k)^{-1} \Sigma_k U_k y \\
&= V_k \Sigma_k^{-1} U^t_k y
\end{align*}
On problem set $5$, you will show that when $k$ is equal to $p$, the last
line is equal to the ordinary least squares solution.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

The variance matrix of the regression vector can be calculated as:
\begin{align*}
\text{Var} (V_k \Sigma_k^{-1} U^t_k y)
  &= \sigma^2 \cdot V_k \Sigma_k^{-1} U^t_k U_k \Sigma_k^{-1}  V_k^t \\
  &= \sigma^2 \cdot V_k \Sigma_k^{-2} V_k^t
\end{align*}
\pause And the trace of this is given by:
\begin{align*}
\text{tr} \left( \text{Var} \widehat{\beta}_{k} \right)
  &= \sigma^2 \cdot \text{tr} \left( V_k \Sigma_k^{-2} V_k^t \right) \\
  &= \sigma^2 \cdot \text{tr} \left( \Sigma_k^{-2} \right) \\
  &= \sum_{i=1}^{k} \frac{\sigma^2}{\sigma_i^2}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Therefore, we have:
\begin{align*}
\text{tr} \left( Var(\widehat{\beta}_1) \right)  \leq
 \text{tr} \left( Var(\widehat{\beta}_2) \right) \leq
 \ldots \leq \text{tr} \left( Var(\widehat{\beta}_p) \right) =
 \text{tr} \left( Var(\widehat{\beta}_{ols}) \right)
\end{align*}
So PCR is another form of variance reduction.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Application of PCR to a single photo}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Ridge vs. PCR: similarities}

\begin{enumerate}
\item Both methods try to reduce variance by using the largest singular values
  of the design matrix $X$.\pause
\item Both have easy to compute, analytic solutions.\pause
\item Efficient method for calculating the solution for multiple values of the
  tuning parameter. PCR is just one regression for all $k$ and ridge uses the
  same SVD decomposition, so each $\lambda$ is just a single matrix multiplication. \pause
\item Both are invariant to rotations of the data matrix $X$ \pause
\item Both are sensitive to the scale and means of the columns $X$; typically a good
  idea to standardize these unless naturally on the same scale to begin with (color
  pixels is one example)
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Ridge vs. PCR: differences}

\begin{enumerate}
\item Ridge smoothly shrinks the singular vectors whereas PCR just throws out the worst\pause
\item Ridge can be fit for any positive lambda, but there are only $p$ possible values for
  the tuning parameter in PCR\pause
\item Ridge is, therefore, preferable when being used with a very small $\lambda$ to
  simply stabilize the solution rather than perform drastic shrinkage
\item Because we do not know whether $\beta$ lives in the first $k$ principal components,
  it is difficult to get any universal results on the bias of PCR.\pause
\item The principal components provide dimension reduction in addition to shrinkage.
  The PCR can be preferable when you have a large number of variables and but want to
  preserve some sort of interpretability.\pause
\item The principal components are also great for visualizations and as inputs in other
  machine learning algorithms.
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{What's next}

Amazingly, we only have $4$ more lectures before Thanksgiving break.

\begin{enumerate}
\item 11-09: Logistic regression revisited
\item 11-11, 11-16, 11-18: Lasso regression
\end{enumerate}

\end{frame}



\end{document}











