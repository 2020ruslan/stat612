\input{../header.tex}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 14 \\\vspace{0.2cm}
PCR and Ridge Regression}\\\vspace{0.5cm}
04 November 2015

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics \\
STAT 312/612
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Notes}

\begin{itemize}
\item problem set 5 posted; due next Wednesday \pause
\item problem set 6 will be due in two weeks, November 18th
  and is also posted (courtesy of DP) \pause
\item the second midterm will be a take-home exam; available online on
  December 2nd, and due December 7th \pause
\item the final problem set, 7, is formally due the last day of
  classes (but we'll accept them through December 14th)
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Goals for today}

\begin{itemize}
\item ridge regression formulation and link to SVD
\item principal component analysis
\item applications to image data
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Ridge regression}

The ridge regression estimator is the solution to the following
modified least squares optimization problem for some value of $\lambda > 0$.
\begin{align*}
\widehat{\beta}_{ridge} &= \argmin_b \left\{ || y - Xb ||_2^2 + \lambda || b ||_2^2 \right\}
\end{align*}
\pause The equation shrinks the coefficients towards zero, adding some bias but
reducing the variance of the estimator.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Ridge regression has an analytical solution. \pause
To see this write the criterion as a matrix equation:
\begin{align*}
(y - Xb)^t (y - Xb)  + \lambda b^t b
&= y^t y + b^t X^t X b - 2 y^t X b + \lambda b^t b \\
\end{align*}
\pause And take its derivative:
\begin{align*}
\frac{\partial}{\partial \beta} \left( y^t y + b^t X^t X b - 2 y^t X b + \lambda b^t b \right)
&= 2 X^t X b - 2 X^t y + 2 \lambda b
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Setting this to zero yields
\begin{align*}
2 X^t X \widehat{\beta} + 2 \lambda \widehat{\beta} &= 2 X^t y \\
(X^t X + I_p \lambda) \widehat{\beta} &=  X^t y \\
\widehat{\beta} &= (X^t X + I_p \lambda)^{-1} \cdot X^t y \\
\end{align*}
\pause This is a useful analytical form, though as with least
squares we would generally not invert the matrix directly but
instead use a stable matrix decomposition.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Now consider the singular value decomposition $U\Sigma V^t$ of
the matrix $X$. We can write the projection matrix $P$ in terms
of this as:
\begin{align*}
P &= X (X^t X)^{-1} X^t \\
&= U \Sigma V^t (V \Sigma^2 V^t)^{-1} V \Sigma U^t \\
&= U \Sigma V^t V \Sigma^{-2} V^t V \Sigma U^t \\
&= U U^t \\
\end{align*}
\pause Remember how important the projection matrix was? This is
a very important result!

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

The analogue of the projection matrix for ridge regression
is given by:
\begin{align*}
P_{\lambda} &= X (X^t X + \lambda I_p)^{-1} X^t \\
\end{align*}
Where $P_{0}$ is equal to the ordinary $P$. \pause As was
the case last time, this matrix maps $y$ into the predicted
values $\hat{y}$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Notice that because $VV^t$ is equal to the identity matrix, we
can write the inner term of this projection matrix in a nice form:
\begin{align*}
X^t X + \lambda I_p &= V \Sigma^2 V^t + \lambda V V^t \\
&= V(\Sigma^2 + \lambda)V^t
\end{align*}
\pause And the inverse is given as:
\begin{align*}
(X^t X + \lambda I_p)^{-1}
&= V(\Sigma^2 + \lambda)^{-1}V^t \\
&= V E_{\lambda} V^t
\end{align*}
\pause Where $E_{\lambda}$ is a diagonal matrix with entries:
\begin{align*}
E_{\lambda} &=
\text{diag} \left( \frac{1}{\sigma^2_{max} + \lambda}, \ldots, \frac{1}{\sigma^2_{min} + \lambda}\right)
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Remember that the condition number is the ratio of the largest and smallest
singular value of a matrix.\pause What is the condition number of $X^tX$?
\begin{align}
\text{cond} (X^t X) &= \frac{\sigma_max^2}{\sigma_min^2}
\end{align}
\pause How about the condition number of $X^t X + \lambda I_p$?
\begin{align}
\text{cond} (X^t X + \lambda I_p) &= \frac{\sigma_max^2 + \lambda}{\sigma_min^2 + \lambda}
\end{align}
\pause How does the incorporation of $\lambda$ change our ability to invert the
matrix?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Back to the projection matrix, what is the decomposition of
$P_\lambda$ in terms of the singular value decomposition? \pause
\begin{align*}
P_\lambda &= X (X^t X + \lambda I_p)^{-1} X^t \\
&= U \Sigma V^t (V^t \Sigma^2 V)^{-1} V \Sigma U^t \\
&= U \Sigma (\Sigma^{2} + \lambda I_p)^{-1} \Sigma U^t \\
&= U D U^t \\
\end{align*}
For the diagonal matrix $D$:
\begin{align}
D &= \text{diag} \left( \frac{\sigma_1^2}{\sigma_1^2 + \lambda}, \ldots, \frac{\sigma_p^2}{\sigma_p^2 + \lambda}  \right)
\end{align}
\pause So we are shrinking in the directions of the singular vectors,
with more shrinkage on the smaller singular values.

\end{frame}



\end{document}











