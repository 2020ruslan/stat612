\input{../header-ho.tex}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 16 \\\vspace{0.2cm}
Solving GLMs via IRWLS}\\\vspace{0.5cm}
09 November 2015

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics \\
STAT 312/612
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Notes}

\begin{itemize}
\item problem set 5 posted; due next class \pause
\item problem set 6, November 18th
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Goals for today}

\begin{itemize}
\item fixed PCA example from last time
\item how to solve logistic regression via weighted least squares
\item classification problem on image corpus
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont fixed PCA example from last time}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont solving logistic regression}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{GLMs}

Recall that we define generalized linear models such that
the mean of $y$ is some function of $X\beta$, rather than
directly equal to it:
\begin{align*}
\mathbb{E} (y | X) &= g^{-1} \left( X \beta \right)
\end{align*}
With $g$, called the \textit{link function}, equal to some
fixed and known function.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Logistic regression}

The logistic regression function uses $g$ equal to the logit
function to describe a distribution with $y \in \{0,1\}$.
Specifically, we have the following description of the statistical
model:
\begin{align*}
\mathbb{E} (y | X) &= \text{logit}^{-1} \left( X \beta \right) \\
&= \frac{1}{1 + e^{- X \beta}}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Now that we have discussed how to solve the ordinary least squares
equation, you may wonder how we would go about solving the logistic
regression problem.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Recall that the likelihood of the data point $y_i$ given its
mean $p_i$ is equal to:
\begin{align*}
L_i(y_i | p_i)
&= \text{exp} \left\{y_i \cdot \log\left(\frac{p_i}{1-p_i}\right) + \log(1 - p_i) \right\} \\
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

There are two terms here that depend on $p_i$ in different ways:
\begin{align*}
L_i(y_i | p_i)
&= \text{exp} \left\{y_i \cdot {\color{solarized@red}\log\left( \frac{p_i}{1-p_i}\right)} + {\color{solarized@blue} \log(1 - p_i)} \right\} \\
\end{align*}
\pause The {\color{solarized@red} first term} is simply equal to the projection
of the regression vector $x_i^t \beta$.

\pause We can write the {\color{solarized@blue} second term} using:
\begin{align*}
p_i &= \frac{1}{1 + e^{-x_i^t \beta}} \\
1 - p_i &= \frac{ e^{-x_i^t \beta}}{1 + e^{-x_i^t \beta}} \\
&=\frac{ 1}{1 + e^{x_i^t \beta}} \\
\log(1 - p_i) &= -1 \cdot \log(1 + e^{x_i^t \beta})
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

We can then write the log-likelihood of the full model, in vector
form, as:
\begin{align*}
l(y_i | x_i, \beta)
&= \sum_i \left(y_i \cdot x_i^t \beta - \log(1 + ^{x_i^t \beta}) \right) \\
&= y^t X \beta - \log(1 + e^{X\beta})
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

To find the MLE estimator, we want to find zeros of the derivative of
the log-likelihood. The derivative is given by:
\begin{align*}
\nabla_\beta l(y_i | x_i, \beta)
&= \nabla y^t X \beta - \nabla \log(1 + e^{X\beta}) \\
&= X^t y - \nabla \log(1 + e^{X\beta})
\end{align*}
\pause The second term can be calculated by writing the gradient out
component wise.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

In other words, we see from a simple application of the chain
rule that:
\begin{align*}
\frac{\partial}{\partial \beta_k} \log(1 + e^{X \beta})
&= \frac{1}{1 + e^{X \beta}} \cdot x_k^t \cdot e^{X \beta}
\end{align*}
\pause Which can be simplified in terms of $p$:
\begin{align*}
\frac{1}{1 + e^{X \beta}} \cdot x_k^t \cdot e^{X \beta}
&= x_k^t p
\end{align*}
\pause In vector form gives the gradient as:
\begin{align*}
\nabla \log(1 + e^{X\beta}) &= X^t p
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

So now we have an explicit form of the gradient of the
log-likelihood:
\begin{align*}
\nabla_\beta l(y_i | x_i, \beta)
&= X^t y - \nabla \log(1 + e^{X\beta}) \\
&= X^t y - X^t p \\
&= X^t (y - p)
\end{align*}

\pause We cannot simply set this to zero because $p$ is a non-linear
function of $X$. However it does tell us that the residual in logistic
regression should be uncorrelated with the $X$ matrix (just as was
the case with linear regression).

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

To find a numerical solution to the zeros of the function
$\nabla_\beta l(y_i | x_i, \beta)$, the Newton–Raphson method
can be used.

\pause The technique is best described in a series of
illustractions.
\begin{quote}
\url{http://euler.stat.yale.edu/~tba3/stat612/lectures/lec16/img/NewtonIteration_Ani.gif}
\end{quote}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

We saw that the scalar version of using Newton–Raphson to determine
the optimal values of $f$ is given by (remember the derivatives are
one more than the illustration because we want critical points rather
than zeros of $f$):
\begin{align*}
\beta^{(k+1)} &= \beta^{(k)} - \frac{f\,'(\beta^{(k)})}{f\,''(\beta^{(k)})}
\end{align*}
\pause The multivariate version of Newton–Raphson applies the following
set of updates:
\begin{align*}
\beta^{(k+1)} &= \beta^{(k)} - H^{-1}(\beta^{(k)}) \nabla f\,(\beta^{(k)})
\end{align*}
Where $H$ is the Hessian matrix of all second order partial derivatives.

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

What is the Hessian of the log-likelihood? The gradient
was $X^t (y - p)$, so we can quickly see that the
Hessian only depends on the term $X^t p$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

In componentwise form, we see that:
\begin{align*}
\frac{\partial^2 l(\beta)}{\partial \beta_j \partial \beta_k}
&= \frac{\partial}{\partial \beta_j} \sum_i x_{i,k} \cdot p_i \\
&= \sum_i x_{i,k} \cdot \frac{\partial p_i}{\partial \beta_j} \\
\end{align*}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

The partial derivative of $p_i$ with respect to $\beta_j$ is
given by the following (it is just calculus with a clever
grouping of terms at the end):
\begin{align*}
\frac{\partial p_i}{\partial \beta_j}
&= \frac{\partial}{\partial \beta_j} \frac{1}{1+e^{-x_i^t \beta}} \\
&= \frac{-1}{(1+e^{-x_i^t \beta})^2} \cdot -1 \cdot x_{i,j} \cdot e^{-x_i^t \beta} \\
&= x_{i,j} \cdot \left(\frac{1}{1+e^{-x_i^t \beta}} \right) \cdot \left(\frac{e^{-x_i^t \beta}}{1+e^{-x_i^t \beta}} \right) \\
&= x_{i,j} \cdot p_i \cdot (1 - p_i) \\
&= x_{i,j} \cdot \text{Var}(y_i)
\end{align*}
\pause We don't actually need the final line, but include it as it
helps to give some intutition to the next set of steps.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

This now yields the Hessian as:
\begin{align*}
H l(y | x_i, \beta) &= - X^t D X
\end{align*}
Where $D$ is an $n$-by-$n$ diagonal matrix with components equal
to $p_i\cdot(1-p_i)$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

And therefore the Newton-Raphson step is given by:
\begin{align*}
\beta^{(k+1)} &= \beta^{(k)} - H^{-1}(\beta^{(k)}) \nabla f\,(\beta^{(k)}) \\
&= \beta^{(k)} + (X^t D X)^{-1} X^t (y - p^{(k)})
\end{align*}
\pause Look familiar?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

We can re-write this, for a suitable $z$, as:
\begin{align*}
\beta^{(k+1)}
&= (X^t D X)^{-1} (X^t D X) \beta^{(k)} + (X^t D X)^{-1} X^t (y - p^{(k)}) \\
&= (X^t D X)^{-1} X^t D (X \beta^{(k)}  + D^{-1} (y - p) ) \\
&= (X^t D X)^{-1} X^t D z
\end{align*}
\pause So solving the logistic regression amounts to solving a sequence of
weighted least squares models:
\begin{align*}
\beta^{(k+1)} &= \argmin_b \left\{ || D^{1/2} (z - Xb) ||_2^2  \right\}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Notice that the scheme weights observations more if they
have predicted probabilities close to $0$ or $1$. \pause
Does this make sense based on our other lecture?

\pause Another interpretation is that it put the highest
weight on points with the lowest variance.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

This method generalizes to generalized linear models with
exponential families, and has some fairly deep connections to
Fischer information.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Why is this important? \pause
\begin{enumerate}
\item Explains many of the results on problem set $4$; in
particular when linear and logistic regression can be expected
to give similar predictions and when they don't. \pause
\item Helps explain and address convergence properties in
generalized linear models. \pause
\item The logistic regression version of the normal equations
can be used to establish large sample theory convergence results. \pause
\item Gives us an idea of how the geometric and analytic concepts underlying
ridge regression, principal component analysis, and (later) lasso regression
connect to generalized linear models.
\end{enumerate}

\end{frame}

\end{document}











