\input{../header.tex}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 06 \\\vspace{0.2cm}
Finite-Sample Properties of OLS}\\\vspace{0.5cm}
21 September 2015

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics \\
STAT 312/612
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Goals for today}

\begin{enumerate}
\item Linear models assumptions
\item OLS Finite sample properties
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Linear models assumptions}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf I. Linearity}

We observe a pair of random variables $(y, X)$, which
have the following relationship for some random vector
$\epsilon$ and fixed vector $\beta$:
\begin{align*}
y &= X \beta + \epsilon
\end{align*}
\pause We assume that the following dimensions hold.
\begin{align*}
y &\in \mathbb{R}^n \\
X &\in \mathbb{R}^{n \times p} \\
\beta &\in \mathbb{R}^p \\
\epsilon &\in \mathbb{R}^n \\
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf II. Strict exogeneity}

For all $X$, we have:
\begin{align}
\mathbb{E} \left( \epsilon | X \right) &= 0
\end{align}
\pause The `strict' part comes from the conditional on all
of $X$.

\pause Notice that this implies the weaker assumption we
used with simple linear models:
\begin{align}
\mathbb{E} \left( \epsilon \right) &= \mathbb{E} \left\{ \mathbb{E} \left( \epsilon | X \right) \right\} \\
&= \mathbb{E} \left\{  0 \right\} \\
&= 0
\end{align}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf III. No multicollinearity}

We have:
\begin{align*}
\mathbb{P} \left[ \text{rank} (X) = p \right] &= 1
\end{align*}
\pause When broken, it is impossible to do inference
on $\beta$ without additional assumptions.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf IV. Spherical errors}

The variance of the errors is given by:
\begin{align*}
\mathbb{V} \left( \epsilon | X \right) &= \sigma^2 \mathbb{I}_n
\end{align*}
Recall that $\mathbb{V} u = \mathbb{E} (u u^t)$.

\pause We can break this assumption into two parts; the
{\it homoscedasticity} assumption:
\begin{align*}
\mathbb{E} ( \epsilon_i^2 ) &= \sigma^2
\end{align*}
and {\it no autocorrelation} assumption:
\begin{align*}
\mathbb{E} ( \epsilon_i \epsilon_j ) &= 0 \quad i \neq j
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf V. Normality}

The final, most restrictive assumption, is that the errors
follow a multivariate normal distribution:
\begin{align*}
\epsilon | X \sim \mathcal{N} (0, \sigma^2 \mathbb{I}_n)
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{center}
{\Large Classical linear model assumptions}
\end{center}

{\bf I. Linearity} $\quad Y = X\beta + \epsilon$

{\bf II. Strict exogeneity} $\quad \mathbb{E} \left( \epsilon | X \right) = 0$

{\bf III. No multicollinearity} $\quad \mathbb{P} \left[ \text{rank} (X) = p \right] = 1$

{\bf IV. Spherical errors} $\quad \mathbb{V} \left( \epsilon | X \right) = \sigma^2 \mathbb{I}_n$

{\bf V. Normality} $\quad \epsilon | X \sim \mathcal{N} (0, \sigma^2 \mathbb{I}_n)$

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Finite sample properties}
\end{flushright}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf Ordinary least squares}

We have already derived the ordinary least square estimator:
\begin{align*}
\widehat{\beta} &= (X^t X)^{-1} X^t y
\end{align*}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

If we define the following values:
\begin{align*}
S_{xx} &= \frac{1}{n} X^t X \\
s_{xy} &= \frac{1}{n} X^t y
\end{align*}
The ordinary least squares estimator can also be written:
\begin{align*}
\widehat{\beta} &=  S_{xx}^{-1} s_{xy}
\end{align*}
A form that will be useful for large sample theory.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf Special matricies}

Last time we defined the following matricies:
\begin{align*}
P &= X (X^t X)^{-1} X^t \\
M &= \mathbb{I}_n - P
\end{align*}
\pause Today we have one more matrix $A$ that does
not have a direct geometric interpretation but is
nonetheless very useful:
\begin{align*}
A &= (X^t X)^{-1} X^t \\
Ay &= \widehat{\beta}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Last time we showed that:
\begin{align*}
P^2 &= P^t = P \\
M^2 &= M^t = M \\
PX &= X \\
MX &= 0 \\
Py &= X\beta \\
My &= M\epsilon = r
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

The matrix $A$ is not square, but the outer product
has a nice property:
\begin{align*}
AA^t &= (X^t X)^{-1} X^t X (X^t X)^{-1} \\
&= (X^t X)^{-1}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf Two final definitions}

The residuals and estimate of the $\sigma^2$ parameter
are given as:
\begin{align*}
r &= y - X\widehat{\beta} \\
s^2 &= \frac{1}{n-p} r^t r
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf Finite sample properties}

Under assumptions I-III:

\hspace{1cm} (A) $\mathbb{E} (\widehat{\beta} | X) = \beta$ \pause

Under assumptions I-IV:

\hspace{1cm}  (B)  $\mathbb{V} (\widehat{\beta} | X) = \sigma^2 (X^t X)^{-1}$ \pause

\hspace{1cm}  (C) $\widehat{\beta}$ is the best linear unbiased estimator (Gauss-Markov) \pause

\hspace{1cm}  (D) $Cov( \widehat{\beta}, r | X) = 0$ \pause

\hspace{1cm}  (E) $\mathbb{E} s^2 = \sigma^2$ \pause

Under assumptions I-V:

\hspace{1cm}  (F) $\widehat{\beta}$ achieves the Cramér–Rao lower bound

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf (A) Unbiased regression estimate $\widehat{\beta}$}

Notice that the error in our estimate can be re-written in terms of
the matrix $A$:
\begin{eqnarray*}
\widehat{\beta}  - \beta &=& (X^t X)^{-1} X^t y - \beta \\ \pause
&=& (X^t X)^{-1} X^t (X\beta + \epsilon) - \beta \\ \pause
&=& (X^t X)^{-1} X^t X \beta + (X^t X)^{-1} X^t \epsilon - \beta \\ \pause
&=& \beta + (X^t X)^{-1} X^t \epsilon - \beta \\
&=& A \epsilon
\end{eqnarray*}
\pause From here, we can derive the unbiased result easily:
\begin{align*}
\mathbb{E} (\widehat{\beta}  - \beta | X) &= \mathbb{E} (A \epsilon | X) \\ \pause
&= A \cdot \mathbb{E} (\epsilon | X) \\
&= 0
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf (B) Form of the variance }

The formula for the variance of the ordinary least squares estimator
can be derived from our assumptions and prior results.
\begin{eqnarray*}
\mathbb{V} ( \widehat{\beta} | X ) &=& \mathbb{V} ( \widehat{\beta} - \beta | X) \\ \pause
&=& \mathbb{V} ( A \epsilon | X) \\ \pause
&=& A \mathbb{V} ( \epsilon | X) A^t \\ \pause
&=& A \mathbb{E} ( \epsilon \epsilon^t | X) A^t \\ \pause
&=& A (\sigma^2 \mathbb{I}_n ) A^t \\ \pause
&=& \sigma^2 A A^t \\ \pause
&=& \sigma^2 (X^{t} X)^{-1}
\end{eqnarray*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\bf (E) Unbiased $s^2$}

We have already established that $r^t r = \epsilon^t M \epsilon$,
so all we need to do is show that
$\mathbb{E} (\epsilon^t M \epsilon | X) = \sigma^2 (n-p)$. \pause

We can write this expected value in terms of the trace of $M$:
\begin{eqnarray*}
\mathbb{E} (\epsilon^t M \epsilon | X)
&=& \sum_{i=1}^n \sum_{j=1}^n m_{i,j} \mathbb{E} (\epsilon_i \epsilon_j | X) \\ \pause
&=& \sum_{i=1}^n m_{i,i} \sigma^2 \\ \pause
&=& \sigma^2 \sum_{i=1}^n m_{i,i} \\ \pause
&=& \sigma^2 tr(M)
\end{eqnarray*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Now we simply need to calculate the trace of $M$:
\begin{eqnarray*}
tr(M) &=& tr(\mathbb{I}_n - P) \\ \pause
&=& tr(\mathbb{I}_n) - tr(P) \\ \pause
&=& n - tr(P)
\end{eqnarray*}
\pause And then,
\begin{eqnarray*}
tr(P) &=& tr(X (X^t X)^{-1} X^t) \\ \pause
&=& tr((X^t X)^{-1} X^t X) \\ \pause
&=& tr( \mathbb{I}_p ) \\ \pause
&=& p
\end{eqnarray*}

\pause Plugging back into the original yields the result.

\end{frame}


\end{document}











