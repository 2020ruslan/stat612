\input{../header.tex}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 20 \\\vspace{0.2cm}
Theory of the Lasso I}\\\vspace{0.5cm}
30 November 2015

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics  \\
STAT 312/612
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

{\color{yaleblue}\fontsize{16pt}{20pt}\selectfont Class Notes}

\begin{itemize}
\item Midterm II - Posted this afternoon, due next Monday
\item Problem Set 7 - Posted Wed., due December 11th (grace period through the 16th)
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Types of bounds}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Recall that the lasso regression replaces the $\ell_2$ penalty with an $\ell_1$ penalty,
and looks deceptively similar to the ridge regression:
\begin{align*}
\widehat{\beta}_\lambda &= \argmin_b \left\{ || y - Xb ||_2^2 + \lambda ||b||_1 \right\}
\end{align*}
Where the $\ell_1$-norm is defined as the sum of the absolute values of the
vector's components:
\begin{align*}
|| \beta ||_1 &= \sum_i | \beta_i |
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

There are three types of errors that we commonly are concerned
with in lasso regression. \pause The prediction loss:
\begin{align*}
|| X (\beta - \widehat{\beta}) ||_2^2
\end{align*}
\pause Parameter estimation:
\begin{align*}
|| \beta - \widehat{\beta} ||_2^2
\end{align*}
\pause And model selection:
\begin{align*}
\text{supp} (\beta) &= \text{supp} (\widehat{\beta})
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

The type of results we want to establish may look different than
you are used to seeing in more introductory courses.
\pause What we will want to be able to construct is a set
$\mathcal{A}$ such that:
\begin{align*}
\mathbb{P} \mathcal{A} &= 1 - \epsilon
\end{align*}
For some small $\epsilon > 0$, where we have bounds such as
\begin{align*}
|| X (\beta - \widehat{\beta}) ||_2^2 &\leq \delta
\end{align*}
Conditioned on being in event $\mathcal{A}$.

\pause Today I'll establish bounds on all three for a simple case
where $X^tX$ is the identity matrix and bounds on the first in the
general case of an arbitrary $X$ matrix.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Simple case}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Simple case}

Let's consider the simple case where $X^t X$ is equal to the identity matrix.

\pause We know that the lasso solution can be written as (Lecture 17):
\begin{align*}
\widehat{\beta}^\lambda_j &= \left\{ \begin{array}{ll}
0, &\lambda > 2 \cdot |x_j^t y| \\[0pt]
x_j^t y - \text{sign}(x_j^t y) \cdot \lambda, &\lambda \leq 2 \cdot|x_j^t y| \\
\end{array} \right.
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Define the set $\mathcal{A}$ such that $\lambda$ bounds
all of the correlations of $X$ with the noise vector $\epsilon$:
\begin{align*}
\left\{ 2 || \epsilon^t X ||_\infty \leq \lambda \right\}
\end{align*}
We will develop a general result later showing what the probability
of $\mathcal{A}$ occurring is.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

We see that for any $j$:
\begin{align*}
2 \cdot|x_j^t y| &= 2 \cdot|x_j^t (X \beta + \epsilon) | \\
&= 2 \cdot | (\beta_j + x_j^t \epsilon) |
\end{align*}
\pause If $\beta_j$ is equal to $0$, then on $\mathcal{A}$:
\begin{align*}
2 \cdot|x_j^t y| &= 2 \cdot | x_j^t \epsilon | \\
&\leq \lambda
\end{align*}
And therefore $\widehat{\beta}_j$ will also be set exactly to zero.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

What about $j$ such that $\beta_j$ is not equal to $0$? \pause Then
on $\mathcal{A}$:
\begin{align*}
2 \cdot|x_j^t y| &= 2 \cdot|x_j^t (X \beta + \epsilon) | \\
&= 2 \cdot |\beta_j + x_j^t \epsilon | \\
&\geq 2 \cdot | \beta_j | - 2 \cdot |x_j^t \epsilon) | \\
&\geq 2 \cdot | \beta_j | - \lambda
\end{align*}
\pause We then have that as long as the following holds:
\begin{align*}
\lambda &\leq 2 \cdot | \beta_j | - \lambda \\
\lambda &\leq | \beta_j |
\end{align*}
$\widehat{\beta}_j$ will be non-zero.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

To establish a bound on the estimation error, we need a bit
more notation. Let $S = \{j \, : \, \beta_j \neq 0\}$, $s$
be the size of the set $S$. Also, let $v_{S}$ be the vector
$v$ which has components not in $S$ set to zero.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

To establish a bound on the estimation error, we need a bit
more notation. Let $S = \{j \, : \, \beta_j \neq 0\}$, $s$
be the size of the set $S$. Also, let $v_{S}$ be the vector
$v$ which has components not in $S$ set to zero.

\pause Further, let:
\begin{align*}
\widehat{\beta}^{\text{oracle}} &= (X^t_S X_S)^{-1} X^t_S y
\end{align*}
That is, the ordinary least squares estimator that knows that
the correct support of $\beta$ is $S$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Now, notice that on $\mathcal{A}$, the estimator
$\widehat{\beta}^\lambda$ will be zero on $S^c$. Therefore:
\begin{eqnarray*}
|| \beta - \widehat{\beta} ||_2^2
 &= || \beta_S - \widehat{\beta}_S ||_2^2 \\ \pause
 &= || \beta_S - \widehat{\beta}^{\text{oracle}}  + \widehat{\beta}^{\text{oracle}} - \widehat{\beta}_S ||_2^2 \\ \pause
 &\leq || \beta_S - \widehat{\beta}^{\text{oracle}} ||_2^2  + || \widehat{\beta}^{\text{oracle}} - \widehat{\beta}_S ||_2^2 \\ \pause
 &= || \beta_S - \widehat{\beta}^{\text{oracle}} ||_2^2  + \sum_{j \in S} | x_j^t y - \text{sign} (x_j^t y) \lambda - x_j^t y | \\ \pause
 &= || \beta_S - \widehat{\beta}^{\text{oracle}} ||_2^2  + \sum_{j \in S} |\text{sign} (x_j^t y) \lambda | \\ \pause
 &= || \beta_S - \widehat{\beta}^{\text{oracle}} ||_2^2  + s \lambda \\
\end{eqnarray*}
\pause So the cost of not knowing $S$ is an extra factor of $s \lambda$ in the prediction error.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

This result is a type of \textit{oracle inequality}, which
relates the error of not knowing some quantity to the error
that can be attained when the quantity is known.

In the lasso literature, this almost always refers to comparing
a penalized estimator on $X$ to ordinary least squares fit on the
set $S$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Now, how far off will the prediction of $\beta$ be? Notice that:
\begin{align*}
|| X (\beta - b) ||_2^2 &= (\beta - b)^t X^t X (\beta - b)\\
&= (\beta - b)^t (\beta - b)\\
&= || \beta - b ||_2^2
\end{align*}
So we simultaneously established a prediction and estimation bound
for the simple lasso regression.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

So now on the set $\widehat{A}$ we have bounds on all three quantities of
interest.

\pause Notice that none of our bounds so far depend on $n$, $p$, or $\sigma^2$,
which may seem quite odd. These are actually bound up in the choice of $\lambda$
and the scale of $X$ (which we set to be $1$).

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Prediction error: general case}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

We want to establish some result about the general lasso solution. We don't
have an analytic form anymore, so where to begin?

\pause Let's start with the one relationship we know to be true between
$\widehat{\beta}$ (which I will call $b$ to simplify the slides) and $\beta$:
\begin{align*}
|| y - Xb ||_2^2 + \blue{\lambda || b ||_1} \leq || y - X\beta ||_2^2 + \blue{\lambda ||\beta||_1}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

By expanding the $\ell_2$-norm and writing $y$ as $X\beta + \epsilon$,
the right-hand side can be written as:
\begin{align*}
|| y - X\beta ||_2^2 + \blue{\lambda ||\beta||_1}
&= y^t y + \beta^t X^t X \beta - 2 y^t X \beta + \blue{\lambda ||\beta||_1} \\
&= y^t y + \beta^t X^t X \beta - 2 \beta^t X^t X \beta - 2 \epsilon^t X \beta + \blue{\lambda ||\beta||_1} \\
&= y^t y - \beta^t X^t X \beta - 2 \epsilon^t X \beta + \blue{\lambda ||\beta||_1}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Similarly, the left hand side simply becomes:
\begin{align*}
|| y - Xb ||_2^2 + \blue{\lambda ||b||_1}
&= y^t y + b^t X^t X b - 2 y^t X b + \blue{\lambda ||b||_1} \\
&= y^t y + b^t X^t X b - 2 \beta^t X^t X b - 2 \epsilon^t X b + \blue{\lambda ||b||_1} \\
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Canceling the $y^t y$ on both sides and putting similar terms on each side
of the inequality, this yields:
\begin{align*}
b^t X^t X b - 2 \beta^t X^t X b - 2 \epsilon^t X b + \blue{\lambda ||b||_1}  &\leq
- \beta^t X^t X \beta - 2 \epsilon^t X \beta + \blue{\lambda ||\beta||_1} \\
b^t X^t X b - 2 \beta^t X^t X b + \beta^t X^t X \beta &\leq 2 \epsilon^t X (b - \beta)
+ \blue{\lambda || \beta ||_1 - \lambda || b ||_1}
\end{align*}
\pause The left hand side can be written as an inner product, and we now have a basic
inequality with three terms:
 \begin{align*}
|| X (\beta - b) ||_2^2 &\leq 2 \epsilon^t X (b - \beta) + \blue{\lambda \cdot \left( || \beta ||_1 - || b ||_1 \right)}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

We see that this decomposes nicely into three distinct terms:
 \begin{align*}
\red{|| X (\beta - b) ||_2^2} &\leq \violet{2 \epsilon^t X (b - \beta)} + \blue{\lambda \cdot \left( || \beta ||_1 - || b ||_1 \right)}
\end{align*}
These are the \red{the loss to be minimized}, the \violet{empirical part}, and \blue{the penalty term}.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\textbf{Bound on inner product $u^t v$}

Let $u$ and $v$ be arbitrary vectors. Notice that:
\begin{align*}
| u^t v | &= \left| \sum_i u_i v_i \right|\\
&\leq  \sum_i |u_i v_i| \\
&\leq \sum_i | \max_i (u_i) v _i | \\
&= |\max_i (u_i)| \cdot \sum_i | v_i |\\
&= || u ||_\infty \cdot || v ||_1
\end{align*}
This is a special case of H\"{o}lder's inequality.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

We will use this trick to bound the empirical part of our
inequality:
\begin{align*}
2 | \epsilon^t X (b - \beta) | \leq 2 || \epsilon^t X ||_\infty \cdot || b - \beta ||_1
\end{align*}
\pause We will then use the same definition of $\mathcal{A}$:
\begin{align*}
\mathcal{A} &= \left\{ 2 || \epsilon^t X ||_\infty \leq \lambda \right\}
\end{align*}
On this set, the random part is on the same order of magnitude as the penalty part.
We will return in a bit to talk about what would make this quantity small.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

On the set $\mathcal{A}$, we have:
\begin{align*}
\red{|| X (\beta - b) ||_2^2} &\leq \violet{2 \epsilon^t X (b - \beta)} + \blue{\lambda \cdot \left( || \beta ||_1 - || b ||_1 \right)}\\
&\leq || 2 \epsilon^t X ||_\infty \cdot || b - \beta ||_1 + \lambda \cdot \left( || \beta ||_1 - || b ||_1 \right)\\
&\leq \lambda || b - \beta ||_1 + \lambda \cdot \left( || \beta ||_1 - || b ||_1 \right)\\
&\leq \lambda \cdot \left\{|| b - \beta ||_1 + || \beta ||_1 - || b ||_1 \right\}
\end{align*}
\pause And using the inequality $||\beta|| + ||b|| \geq || \beta - b ||$, we then have:
\begin{align*}
\red{|| X (\beta - b) ||_2^2}
&\leq 2 \lambda || \beta ||_1
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

So, it seems that picking a smaller $\lambda$ yields a tighter bound. However,
the probability of being on $\mathcal{A}$ decreases as $\lambda$ decreases.

There is ultimately a trade off to be made, and we will see that it is
possible to parameterize $\lambda$ in a nice way to show-off the various bounds.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Bounds on empirical process}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Now, let's consider when the event $\mathcal{A}$ occurs.
We will assume that the columns of $X_j$ have a norm of $1$.

Start by setting $z_j$ equal to $\epsilon^t X_j$. These are
distributed (not necessarily independent) as
$\mathcal{N}(0, \sigma^2)$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

To bound these probabilities, recall this result on the
maximum of $p$ observations from identically distributed
normal random variables:
\begin{align*}
\mathbb{E} \left[ \max_j | z_j | \right] &\leq \sigma \sqrt{2 \log(2p)}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Using the Markov inequality, we get for any $a>0$:
\begin{align*}
\mathbb{P} \left[\max_j | z_j | \geq a / 2 \right]
 &\leq \frac{\mathbb{E} \max_j | z_j |}{a / 2} \\
 &\leq \frac{\sigma \sqrt{8 \log(2p)}}{a}
\end{align*}
\pause Now, for simplicity, set $a = A \cdot \sqrt{8 \log(2p) \sigma^2}$
for any $A > 1$. Then $\mathbb{P} \mathcal{A} = 1 - A^{-1}$ whenever
\begin{align*}
\lambda \geq A \cdot \sqrt{8 \log(2p) \sigma^2}
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Summary and consistency}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

So far, I have been solving the following optimization problem:
\begin{align*}
\widehat{\beta}_\lambda &= \argmin_b \left\{ || y - Xb ||_2^2 + \lambda ||b||_1 \right\}
\end{align*}
It will be useful now to re-scale the problem.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

Up until now we have assumed that $X^tX$ has $1$'s on the diagonal. A more natural
assumption is that it was $n$'s on the diagonal. Let $X' = \sqrt{n} \cdot X$ and
$\beta' = n^{-1/2} \beta$. Then, the new criterion becomes:
\begin{align*}
\cdot || y - X'b' ||_2^2 + \lambda \sqrt{n} \cdot ||b'||_1 \\
\cdot || y - X'b' ||_2^2 + \lambda' \cdot ||b'||_1
\end{align*}
So, therefore we can solve this re-scaled problem by simply dividing all of the
$\lambda$ parameters by $\sqrt{n}$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

So we now can put all of this in a single theorem. \pause

\textbf{Theorem} For some $A > 1$ and all $\lambda > A \cdot \sqrt{8 n^{-1} \log(2p) \sigma^2}$,
we have with probability $1 - A^{-1}$:
\begin{align*}
\red{|| X (\beta - b) ||_2^2} / n &\leq 2 \frac{\lambda}{\sqrt{n}} || \beta ||_1
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

How do we get consistency out of this? We need both of these things to
go to zero as $n$ goes to infinity:
\begin{align*}
A^{-1} \rightarrow 0 \\
A \cdot \sqrt{\frac{8 \log(2p) \sigma^2}{n}} || \beta ||_1 \rightarrow 0 \\
\end{align*}
\pause Notice that as long as:
\begin{align*}
\sqrt{\frac{8 \log(2p) \sigma^2}{n}} || \beta ||_1 \rightarrow 0 \\
\end{align*}
We can set $A$ to decay very slowly, such as $A = n^{-0.01}$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}

So, assuming that $\sigma^2$ and the size of the true $\beta$ stay fixed,
we have consistency of the lasso estimator as long as $p_n$ is dominated
by $e^n$.

\end{frame}

\end{document}











